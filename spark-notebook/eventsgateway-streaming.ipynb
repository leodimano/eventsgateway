{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkPackages = [\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\",\n",
    "    \"org.apache.spark:spark-avro_2.12:3.5.1\",\n",
    "    \"org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.1\",\n",
    "    \"org.apache.kafka:kafka-clients:2.8.2\",\n",
    "    \"org.apache.kafka:kafka_2.13:2.8.2\"\n",
    "]\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ingest-kafka-data\")\n",
    "    .config('spark.jars.packages', \",\".join(sparkPackages))\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"sv-uploads-default-topic\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = (\n",
    "    df\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"checkpointLocation\", \"/tmp/\")\n",
    "    .toTable(\"default.kafka_stream_events\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 3d22317a-8455-45e7-9765-b30b46da7cb9, runId = 4261f9a5-b1ef-42ca-ab69-dd49ca8611c1] terminated with exception: Failed to create new KafkaAdminClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessAllAvailable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:347\u001b[0m, in \u001b[0;36mStreamingQuery.processAllAvailable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocessAllAvailable\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    Blocks until all available data in the source has been processed and committed to the\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    sink. This method is intended for testing.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    >>> sq.stop()\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessAllAvailable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 3d22317a-8455-45e7-9765-b30b46da7cb9, runId = 4261f9a5-b1ef-42ca-ab69-dd49ca8611c1] terminated with exception: Failed to create new KafkaAdminClient"
     ]
    }
   ],
   "source": [
    "stream.processAllAvailable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.kafka_stream_events\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.avro._\n",
    "import java.nio.file.{Files, Paths}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import scala.collection.immutable.Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafka_topic = sv-uploads-default-topic\n",
       "kafka_brokers = kafka:9092\n",
       "kafka_from = earliest\n",
       "kafka_maxoffsets = 10000\n",
       "kafka_maxfiles = 1\n",
       "s3_bucket = eventsgateway-local\n",
       "environment = development\n",
       "jsonFormatSchema = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"namespace\": \"com.tfgco.eventsgateway\",\n",
       "  \"type\": \"record\",\n",
       "  \"name\": \"Event\",\n",
       "  \"fields\": [\n",
       "    {\n",
       "      \"name\": \"id\",\n",
       "      \"type\": \"string\"\n",
       "    },\n",
       "    {\n",
       "      \"name\": \"name\",\n",
       "      \"type\": \"string\"\n",
       "    },\n",
       "    {\n",
       "      \"name\": \"props\",\n",
       "      \"default\": {},\n",
       "      \"type\": {\n",
       "        \"type\": \"map\",\n",
       "        \"values\": \"string\"\n",
       "      }\n",
       "    },\n",
       "    {\n",
       "      \"name\": \"serverTimestamp\",\n",
       "      \"type\": \"long\"\n",
       "    },\n",
       "    {\n",
       "      \"name\": \"clientTimestamp\",\n",
       "      \"type\": \"long\"\n",
       "    }\n",
       "  ]\n",
       "}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val kafka_topic = \"sv-uploads-default-topic\"\n",
    "if (kafka_topic == \"\") {\n",
    "      throw new IllegalStateException(\"topic variable is empty!\");\n",
    "}\n",
    "\n",
    "val kafka_brokers = \"kafka:9092\"\n",
    "val kafka_from = \"earliest\"\n",
    "val kafka_maxoffsets = \"10000\"\n",
    "val kafka_maxfiles = \"1\"\n",
    "val s3_bucket = \"eventsgateway-local\"\n",
    "\n",
    "val jsonFormatSchema : String = \"\"\"{\n",
    "  \"namespace\": \"com.tfgco.eventsgateway\",\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Event\",\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"name\": \"id\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"name\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"props\",\n",
    "      \"default\": {},\n",
    "      \"type\": {\n",
    "        \"type\": \"map\",\n",
    "        \"values\": \"string\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"serverTimestamp\",\n",
    "      \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"clientTimestamp\",\n",
    "      \"type\": \"long\"\n",
    "    }\n",
    "  ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input_df = [key: binary, value: binary ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input_df = spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"maxFilesPerTrigger\", kafka_maxfiles)\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_brokers)\n",
    "  .option(\"subscribe\", kafka_topic)\n",
    "  .option(\"startingOffsets\", kafka_from)\n",
    "  .option(\"maxOffsetsPerTrigger\", kafka_maxoffsets)\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [id: string, name: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, name: string ... 6 more fields]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = input_df\n",
    "    .withColumn(\"event\", from_avro($\"value\", jsonFormatSchema))\n",
    "    .withColumn(\"id\", $\"event.id\")\n",
    "    .withColumn(\"name\", $\"event.name\")\n",
    "    .withColumn(\"props\", $\"event.props\")\n",
    "    .withColumn(\"clienttimestamp\", $\"event.clientTimestamp\")\n",
    "    .withColumn(\"servertimestamp\", $\"event.serverTimestamp\")\n",
    "    .withColumn(\"date\", to_date(from_unixtime($\"event.clientTimestamp\" / 1000)))\n",
    "    .withColumn(\"year\", date_format($\"date\", \"YYYY\"))\n",
    "    .withColumn(\"month\", date_format($\"date\", \"MM\"))\n",
    "    .withColumn(\"day\", date_format($\"date\", \"dd\"))\n",
    "    .select(\"id\", \"name\", \"props\", \"clienttimestamp\", \"servertimestamp\", \"year\", \"month\", \"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hadoop_conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, __spark_hadoop_conf__.xml\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, __spark_hadoop_conf__.xml"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hadoop_conf = spark.sparkContext.hadoopConfiguration\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "// hadoop_conf.set(\"fs.s3a.endpoint\", \"http://localstack:4572/eventsgateway-local\")\n",
    "// hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"dummy\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"dummy\")\n",
    "hadoop_conf.set(\"fs.s3a.proxy.host\", \"eventsgatewaylocalproxy\")\n",
    "hadoop_conf.set(\"fs.s3a.proxy.port\", \"80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@704f7333\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@704f7333"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query = df\n",
    "    .writeStream\n",
    "    .format(\"orc\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"path\", s\"s3a://$s3_bucket/var/standard/$kafka_topic/data/\")\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints\")\n",
    "    // .option(\"checkpointLocation\", s\"s3a://$s3_bucket/var/standard/checkpoints/$kafka_topic/\") // databricks only\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE DATABASE IF NOT EXISTS eventsgateway_local LOCATION 's3a://eventsgateway-local/var/standard/databases/' \n",
      "\n",
      "CREATE EXTERNAL TABLE IF NOT EXISTS `eventsgateway_local.sv_uploads_default_topic` (`id` string, `name` string, `props` map<string,string>, `servertimestamp` bigint, `clienttimestamp` bigint) PARTITIONED BY (`year` STRING, `month` STRING, `day` STRING) STORED AS ORC LOCATION 's3a://eventsgateway-local/var/standard/sv-uploads-default-topic/data/';\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dbname = eventsgateway_local\n",
       "tablename = sv_uploads_default_topic\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sv_uploads_default_topic"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dbname = s3_bucket.replace('-', '_')\n",
    "val tablename = kafka_topic.replace('-', '_')\n",
    "\n",
    "// RUN THIS INSIDE HIVE ITSELF THROUGH `ocker exec -it hive_hive-server_1 sh -c \"/opt/hive/bin/beeline -u jdbc:hive2://localhost:10000\"`\n",
    "\n",
    "println(s\"\"\"CREATE DATABASE IF NOT EXISTS $dbname LOCATION 's3a://$s3_bucket/var/standard/databases/' \"\"\")\n",
    "println()\n",
    "println(s\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS `$dbname.$tablename` (`id` string, `name` string, `props` map<string,string>, `servertimestamp` bigint, `clienttimestamp` bigint) PARTITIONED BY (`year` STRING, `month` STRING, `day` STRING) STORED AS ORC LOCATION 's3a://$s3_bucket/var/standard/$kafka_topic/data/';\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "customDeps": [
   "com.databricks:spark-avro_2.10:4.0.0"
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
