{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkPackages = [\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\",\n",
    "    \"org.apache.spark:spark-avro_2.12:3.5.1\",\n",
    "    \"org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.1\",\n",
    "    \"org.apache.kafka:kafka-clients:2.8.2\",\n",
    "    \"org.apache.kafka:kafka_2.13:2.8.2\",\n",
    "    \"io.delta:delta-spark_2.12:3.2.0\"\n",
    "]\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ingest-kafka-data\")\n",
    "    .config('spark.jars.packages', \",\".join(sparkPackages))\n",
    "    .config('spark.sql.extensions','io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog','org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_bootstrap_server = \"kafka:9092\"\n",
    "kafka_topic = \"sv-uploads-default-topic\"\n",
    "target_table = \"default.events_delta\"\n",
    "json_avro_schema = \"\"\"{\n",
    "  \"namespace\": \"com.tfgco.eventsgateway\",\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Event\",\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"name\": \"id\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"name\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"props\",\n",
    "      \"default\": {},\n",
    "      \"type\": {\n",
    "        \"type\": \"map\",\n",
    "        \"values\": \"string\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"serverTimestamp\",\n",
    "      \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"clientTimestamp\",\n",
    "      \"type\": \"long\"\n",
    "    }\n",
    "  ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_bootstrap_server) \\\n",
    "  .option(\"subscribe\", kafka_topic) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"kafka.group_id\", \"stream-spark-kafka\") \\\n",
    "  .option(\"includeHeaders\", True) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, to_date, from_unixtime, col\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "\n",
    "df = (\n",
    "    source_df\n",
    "    .withColumn(\"event\", from_avro(\"value\", json_avro_schema))\n",
    "    .withColumn(\"id\", col(\"event.id\"))\n",
    "    .withColumn(\"name\", col(\"event.name\"))\n",
    "    .withColumn(\"props\", col(\"event.props\"))\n",
    "    .withColumn(\"clienttimestamp\", col(\"event.clientTimestamp\"))\n",
    "    .withColumn(\"servertimestamp\", col(\"event.serverTimestamp\"))\n",
    "    .withColumn(\"date\", to_date(from_unixtime(col(\"event.clientTimestamp\") / 1000)))\n",
    "    .withColumn(\"year\", date_format(\"date\", \"yyyy\"))\n",
    "    .withColumn(\"month\", date_format(\"date\", \"MM\"))\n",
    "    .withColumn(\"day\", date_format(\"date\", \"dd\"))\n",
    "    .select(\"id\", \"name\", \"props\", \"clienttimestamp\", \"servertimestamp\", \"year\", \"month\", \"day\")\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(target_table):\n",
    "    spark.catalog.createTable(tableName=target_table, schema=df.schema, souce='delta', path=f\"/tmp/{target_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = (\n",
    "    df\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"checkpointLocation\", \"/tmp/\")\n",
    "    .toTable(target_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+---------------+---------------+----+-----+---+\n",
      "|                  id|      name|               props|clienttimestamp|servertimestamp|year|month|day|\n",
      "+--------------------+----------+--------------------+---------------+---------------+----+-----+---+\n",
      "|479958b3-157a-4dd...|test-event|{some-prop -> som...|  1718381790997|  1718381791007|2024|   06| 14|\n",
      "|cc4871a6-da30-455...|test-event|{some-prop -> som...|  1718381794469|  1718381794473|2024|   06| 14|\n",
      "|cf7dff53-34e0-45d...|test-event|{some-prop -> som...|  1718381797699|  1718381797703|2024|   06| 14|\n",
      "+--------------------+----------+--------------------+---------------+---------------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from {target_table}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "customDeps": [
   "com.databricks:spark-avro_2.10:4.0.0"
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
